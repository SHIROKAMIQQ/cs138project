{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab74378",
   "metadata": {},
   "source": [
    "# Team Members and Contributions\n",
    "![Contributions](https://github.com/SHIROKAMIQQ/cs138project/blob/main/contributions.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed52e1",
   "metadata": {},
   "source": [
    "# Pledge\n",
    "\n",
    "![Pledge](https://github.com/SHIROKAMIQQ/cs138project/blob/main/pledge.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1c52e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Due to its position in the Ring of Fire, the Philippines experiences a great number of earthquakes every year, causing severe structural damage and loss of lives. As such, finding a way to model earthquakes in the country can help in identifying where the most vulnerable parts are and preparing for the next incident.\n",
    "\n",
    "For this project, we will be mapping reports of earthquake magnitude and intensity across various parts of the country during the Magnitude 7.6 Earthquake in Davao that occurred on December 2023.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656e952",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "The project can be divided onto three parts,\n",
    "- Data preprocessing\n",
    "- Synthetic Data Generation\n",
    "- Vulnerability Assessment\n",
    "- Accuracy Testing through Spatio-temporal Analysis\n",
    "\n",
    "## Data Gathering and Preprocessing\n",
    "\n",
    "The project will use three datasets for the mapping of the earthquake: the Did You Feel It Survey from the United States Geological Survey (USGS), the Android alerts from the Earthquake Notification Service sourced from Zenodo, as well as the PHIVOLCS Earthquake Bulletins.\n",
    "\n",
    "We first condense the USGS dataset to a period with the most earthquakes\n",
    "\n",
    "### Condensing by Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgs = pd.read_csv(\"https://raw.githubusercontent.com/SHIROKAMIQQ/cs138project/refs/heads/main/USGS_DATA.csv\")\n",
    "df_usgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgs['time'] = df_usgs['time'].str[:10]\n",
    "df_usgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dfee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_usgs['time'].value_counts()\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b584f4dc",
   "metadata": {},
   "source": [
    "Plot the dataframe to observe trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ece12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(df_counts, x = df_counts.index, y = df_counts, title='Number of Philippine Earthquakes')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53eb44",
   "metadata": {},
   "source": [
    "From the seen trend, the data shall be condensed onto the earthquakes that had only occurred during the time period December 2, 2023 until December 12, 2023.\n",
    "\n",
    "This time range was considered as it contained the most amount of earthquakes during a 10-day period condensed on that area, with 678 recorded latitude-longitude points.\n",
    "\n",
    "This period was also considered from the continual magnitude 4-6 earthquakes that occurred during the said 10-day period after the initial 7.6 magnitude earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_12 = df_usgs[((df_usgs['time'] > '2023-12-01') & (df_usgs['time'] < '2023-12-13')) & df_usgs['place'].str.contains('Philippines')]\n",
    "df_2023_12.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ac66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_12.to_csv('202312Dataset.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc63349",
   "metadata": {},
   "source": [
    "### Condensing by Space\n",
    "#### Spatial Clustering of Relevant Points\n",
    "\n",
    "From this, we had then spatially clustered the dataset from USGS. This is done by taking the haversine distance of each point/row and applying the DBSCAN clustering algorithm (Boeing, 2014), where the maximum distance between neighbors is 30 kilometers. Within this, we also fitted the cluster into a square, which will help us make a square matrix for further processes. \n",
    "\n",
    "This is to be used for the area to be compared to the L by L matrix made by the Synthetic Data Later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"202312Dataset.csv\"\n",
    "OUTPUT_FILE = \"202312Spacial.csv\"\n",
    "\n",
    "# This acts as the threshold for considering how close cells must be to each other\n",
    "#  to be considered part of the same neighborhood/cluster.\n",
    "EPS_KM = 30\n",
    "EARTH_RADIUS_KM = 6371.0\n",
    "EPS_RAD = EPS_KM / EARTH_RADIUS_KM\n",
    "\n",
    "# Load .csv file\n",
    "# This assumes dyfi for now. \n",
    "csv_file = INPUT_FILE  # replace with your file path\n",
    "df = pd.read_csv(csv_file)\n",
    "coords = df[['latitude', 'longitude']].to_numpy()\n",
    "\n",
    "# DBSCAN clustering using Haversine distance\n",
    "# https://stackoverflow.com/questions/24762435/clustering-geo-location-coordinates-lat-long-pairs-using-kmeans-algorithm-with\n",
    "# https://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/ \n",
    "coords_rad = np.radians(coords)  # lat/lon in radians\n",
    "db = DBSCAN(eps=EPS_RAD, min_samples=3, metric='haversine')\n",
    "# print(\"DB:\", db)\n",
    "labels = db.fit_predict(coords_rad)\n",
    "# print(\"labels:\", labels)\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# Find largest cluster/neighborhood\n",
    "unique_labels = [l for l in set(labels) if l != -1]  # exclude noise\n",
    "if not unique_labels:\n",
    "    raise ValueError(\"No clusters found. Try increasing eps or decreasing min_samples.\")\n",
    "cluster_sizes = {l: np.sum(labels == l) for l in unique_labels}\n",
    "largest_cluster_label = max(cluster_sizes, key=cluster_sizes.get)\n",
    "print(f\"Largest cluster: {largest_cluster_label} with {cluster_sizes[largest_cluster_label]} points\")\n",
    "\n",
    "# Extract rows of largest cluster to dyfi_largest_cluster.csv\n",
    "largest_cluster_rows = df[df['Cluster'] == largest_cluster_label]\n",
    "largest_cluster_rows.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Saved largest cluster to {OUTPUT_FILE}\")\n",
    "\n",
    "# Compute square in terms of lat/lon\n",
    "lat_min = largest_cluster_rows['latitude'].min()\n",
    "lat_max = largest_cluster_rows['latitude'].max()\n",
    "lon_min = largest_cluster_rows['longitude'].min()\n",
    "lon_max = largest_cluster_rows['longitude'].max()\n",
    "lat_range = lat_max - lat_min\n",
    "lon_range = lon_max - lon_min\n",
    "max_range = max(lat_range, lon_range)\n",
    "\n",
    "# Build square bounding box (equal sides)\n",
    "lat_mid = (lat_max + lat_min)/2\n",
    "lon_mid = (lon_max + lon_min)/2\n",
    "half_size = max_range/2\n",
    "\n",
    "square_lat_min = lat_mid - half_size\n",
    "square_lat_max = lat_mid + half_size\n",
    "square_lon_min = lon_mid - half_size\n",
    "square_lon_max = lon_mid + half_size\n",
    "\n",
    "print(\"Square bounding box (lat/lon degrees):\")\n",
    "print(f\"latitude:  {square_lat_min:.4f} to {square_lat_max:.4f}\")\n",
    "print(f\"longitude: {square_lon_min:.4f} to {square_lon_max:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "colors = plt.cm.get_cmap('tab20', len(unique_labels))\n",
    "\n",
    "for k in unique_labels:\n",
    "    mask = (labels == k)\n",
    "    xy = coords[mask]\n",
    "    plt.scatter(xy[:,1], xy[:,0], label=f'Cluster {k}')\n",
    "\n",
    "# noise points\n",
    "mask_noise = (labels == -1)\n",
    "plt.scatter(coords[mask_noise,1], coords[mask_noise,0], c='k', marker='x', label='Noise')\n",
    "\n",
    "# overlay square of largest cluster\n",
    "square_lon = [square_lon_min, square_lon_max, square_lon_max, square_lon_min, square_lon_min]\n",
    "square_lat = [square_lat_min, square_lat_min, square_lat_max, square_lat_max, square_lat_min]\n",
    "plt.plot(square_lon, square_lat, 'r-', linewidth=2, label='Bounding Square (Largest Cluster)')\n",
    "\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.title('DBSCAN Clustering of DYFI Responses with Square Bounding Box')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e51b98",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation\n",
    "\n",
    "A modified Olami-Feder-Christensen (OFC) model will be used to simulate synthetic earthquakes and produce data from different simulated intensities, adapted from (Greco et al., 2019) and (Ferreira et al., 2022). Which is a cellular automaton model that starts with an $L \\times L$ matrix wherein for each cell (i, j) inside the matrix, there corresponds some seismologic force $F$. This aims to mimic the uniform motion of tectonic plates, the $F_{t h}$ is a threshold value that indicates the limit of the friction force.\n",
    "\n",
    "This said model is said to reproduce the statistical features of earthquakes which shall be suited for vulnerability assessment for urban areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf03529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from random import random, randint\n",
    "from math import log\n",
    "\n",
    "class OFCModel:\n",
    "    def __init__(self, L: int, ALPHA: float, F_TH: float, f_0: list[list[float]], p: int) -> None:\n",
    "        self.L = L\n",
    "        self.ALPHA = ALPHA\n",
    "        self.F_TH = F_TH\n",
    "        self.f_0 = self._deepcopy_2d(f_0)\n",
    "        self.p = p\n",
    "        self.adj: dict[tuple[int, int], list[tuple[int, int]]] = dict()\n",
    "\n",
    "        for i in range(L):\n",
    "            for j in range(L):\n",
    "                self.adj[(i,j)] = []\n",
    "                for (di,dj) in ((-1,0),(1,0),(0,-1),(0,1)):\n",
    "                    ni, nj = i+di, j+dj\n",
    "                    if 0 <= ni < L and 0 <= nj < L:\n",
    "                        self.adj[(i,j)].append((ni,nj))\n",
    "    \n",
    "    def _deepcopy_2d(self, f: list[list[float]]) -> list[list[float]]:\n",
    "        return [[cell for cell in row] for row in f]\n",
    "    \n",
    "    def _deepcopy_list_tuple(self, lt: list[tuple[int, int]]):\n",
    "        return [(i, j) for i, j in lt]\n",
    "\n",
    "    def rewire(self, adj: dict[tuple[int, int], list[tuple[int, int]]]):\n",
    "        for i, j in adj:\n",
    "            pi = randint(0, 99)\n",
    "            if pi < self.p:\n",
    "                index_to_change = randint(0, len(adj[(i, j)]) - 1)\n",
    "\n",
    "                new_neighbour = (randint(0, self.L - 1), randint(0, self.L - 1))\n",
    "\n",
    "                # dont connect to urself\n",
    "                while new_neighbour == (i, j):\n",
    "                    new_neighbour = (randint(0, self.L - 1), randint(0, self.L - 1))\n",
    "\n",
    "                adj[(i, j)][index_to_change] = new_neighbour\n",
    "    \n",
    "    def get_max_cell(self, f: list[list[float]]):\n",
    "        return max([max(row) for row in f])\n",
    "\n",
    "    # gives a list[LxL], multiple calls allowed to simulate different alpha and f_th\n",
    "    def simulate(self, STEPS: int):\n",
    "        f = self._deepcopy_2d(self.f_0)\n",
    "        adj = {(i, j): self._deepcopy_list_tuple(self.adj[(i, j)]) for i, j in self.adj}\n",
    "\n",
    "        # Open boundary conditions\n",
    "        for i in range(self.L):\n",
    "            f[0][i] = 0\n",
    "            f[self.L - 1][i] = 0\n",
    "            f[i][0] = 0\n",
    "            f[i][self.L - 1] = 0\n",
    "\n",
    "        step_list: list[list[list[float]]] = [self._deepcopy_2d(f)] # add initial as a frame\n",
    "        magnitudes: list[float] = [0]\n",
    "        sizes: list[int] = [0]\n",
    "\n",
    "        for step in range(STEPS):\n",
    "            print(f\"===== STEP {step} =====\")\n",
    "            active_queue = deque()\n",
    "\n",
    "            max_cell = self.get_max_cell(f)\n",
    "            \n",
    "            delta = self.F_TH - max_cell\n",
    "\n",
    "            for i in range(self.L):\n",
    "                for j in range(self.L):\n",
    "                    f[i][j] = min(self.F_TH, f[i][j] + delta)\n",
    "                    if f[i][j] >= self.F_TH:\n",
    "                        active_queue.append((i,j))\n",
    "\n",
    "            earthquake_size = 0\n",
    "            while len(active_queue) > 0:\n",
    "                (ui, uj) = active_queue.popleft()\n",
    "                fi = f[ui][uj]\n",
    "\n",
    "                # f[ui][uj] = 0\n",
    "\n",
    "                for (vi, vj) in adj[(ui,uj)]:\n",
    "                    if f[vi][vj] < self.F_TH:\n",
    "                        f[vi][vj] = min(f[vi][vj]+self.ALPHA*fi, self.F_TH)\n",
    "                        if f[vi][vj] >= self.F_TH:\n",
    "                            active_queue.append((vi,vj))\n",
    "                earthquake_size += 1\n",
    "\n",
    "            m = 0\n",
    "\n",
    "            if earthquake_size > 0:\n",
    "                print(f\"EARTHQUAKE OF SIZE {earthquake_size} OCCURRED\")\n",
    "                m = log(earthquake_size)\n",
    "                print(f\"Earthquake Magnitude {m}\")\n",
    "            \n",
    "            for i in range(self.L):\n",
    "                for j in range(self.L):\n",
    "                    if f[i][j] >= self.F_TH:\n",
    "                        f[i][j] = 0\n",
    "\n",
    "            step_list.append(self._deepcopy_2d(f))\n",
    "            sizes.append(earthquake_size)\n",
    "            magnitudes.append(m)\n",
    "\n",
    "            # rewiring at random\n",
    "            self.rewire(adj)\n",
    "        \n",
    "        \n",
    "        return step_list, magnitudes, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fbe3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramaters based on Greco et al.\n",
    "\n",
    "L = 40\n",
    "ALPHA = 0.21\n",
    "F_TH = 1\n",
    "STEPS = 2000\n",
    "# set to random values initially\n",
    "f: list[list[float]] = [[random() for _ in range(L)] for _ in range(L)]\n",
    "\n",
    "ofc_random = OFCModel(L, ALPHA, F_TH, f, 2)\n",
    "steps, magnitudes, sizes = ofc_random.simulate(STEPS)\n",
    "\n",
    "print(max(magnitudes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe0117",
   "metadata": {},
   "source": [
    "### Plot the generated model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_frames(steps: list[list[list[float]]], magnitudes: list[float]):\n",
    "    frames = [\n",
    "        go.Frame(data=go.Heatmap(z=s, zmin=0, zmax=1), name=f\"Step {i}, Magnitude {magnitudes[i]}\")\n",
    "        for i, s in enumerate(steps)\n",
    "    ]\n",
    "\n",
    "    return go.Figure(data=frames[0].data, frames=frames).update_layout(\n",
    "        updatemenus=[\n",
    "            {\n",
    "                \"buttons\": [{\"args\": [None, {\"frame\": {\"duration\": 100, \"redraw\": True}}],\n",
    "                            \"label\": \"Play\", \"method\": \"animate\",},\n",
    "                            {\"args\": [[None],{\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                            \"mode\": \"immediate\", \"transition\": {\"duration\": 0},},],\n",
    "                            \"label\": \"Pause\", \"method\": \"animate\",},],\n",
    "                \"type\": \"buttons\",\n",
    "            }\n",
    "        ],\n",
    "        # iterate over frames to generate steps... NB frame name...\n",
    "        sliders=[{\"steps\": [{\"args\": [[f.name],{\"frame\": {\"duration\": 0, \"redraw\": True},\n",
    "                                                \"mode\": \"immediate\",},],\n",
    "                            \"label\": f.name, \"method\": \"animate\",}\n",
    "                            for f in frames],}],\n",
    "        height=800,\n",
    "        width=800,\n",
    "        title_x=0.5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeebb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frames(steps, magnitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_size =  pd.DataFrame.from_dict({\n",
    "    'seismic_event': [-1] + [i for i in range(STEPS)],\n",
    "    'size': sizes,\n",
    "})\n",
    "\n",
    "fig = px.bar(df_size, x='seismic_event', y='size')\n",
    "fig.update_traces(marker_line_width=0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b50329",
   "metadata": {},
   "source": [
    "## Vulnerability Assessment\n",
    "\n",
    "The synthetic data generated by the modified OFC model shall give a size $S$ which is the energy released by the entire seismic event, this is determined through the total amount of 'activated' $F_{th} = 1$ sites during the simulation.\n",
    "\n",
    "Adapting the methods of Greco et al., (2019)\n",
    "Since S is the energy of the event, the Magnitude $M$ can be determined through\n",
    "$ M = \\ln{S} $ From (Greco et al., 2019), this can be converted to Macroseismic intensity through, $I(M) = 1.71 M - 1.02$\n",
    "\n",
    "With this, with we can use the intensity and refer to the Modified Mercalli Scale to determine the amount of damage that will be experienced towards that area over the span of 10 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083731df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magnitudes =  pd.DataFrame.from_dict({\n",
    "    'seismic_event': [-1] + [i for i in range(STEPS)],\n",
    "    'magnitude': magnitudes,\n",
    "})\n",
    "\n",
    "fig = px.bar(df_magnitudes, x='seismic_event', y='magnitude')\n",
    "fig.update_traces(marker_line_width=0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intensities =  pd.DataFrame.from_dict({\n",
    "    'seismic_event': [-1] + [i for i in range(STEPS)],\n",
    "    'intensity': [max(0, int(1.71 * magnitude - 1.02)) for magnitude in magnitudes], # 0 for negative intensities\n",
    "})\n",
    "\n",
    "fig = px.bar(df_intensities, x='seismic_event', y='intensity')\n",
    "fig.update_traces(marker_line_width=0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea63e30",
   "metadata": {},
   "source": [
    "## Accuracy Testing through Spatio-temporal Analysis\n",
    "\n",
    "To determine the accuracy of the synthetic data, temporal and spatial analysis will be performed through by generating a probability distribution for both groups of data.\n",
    "\n",
    "To generate the probability distribution, the following formulae will be followed which were based on the integral form of Tsallis' entropy (Ferreira et al., 2022).\n",
    "\n",
    "### For distance\n",
    "\n",
    "$ P_≥(r) = \\int_r^\\infty P_q (r') d r' = e_q (-\\beta_s r) $\n",
    "\n",
    "\n",
    "Where $\\beta_s$ is a scale constant. And $r$ is the distance of some earthquake from the epicenter.\n",
    "\n",
    "### For time\n",
    "\n",
    "$ P_≥(t) = \\int_r^\\infty P_q (t') d t' = e_q (-\\beta_s t) $\n",
    "\n",
    "With $\\beta_t$ as a scale constant. And $t$ is the time interval between two earthquakes (calm time) (Ferreira et al., 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f2a6ae",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "So far we've been able to generate the Synthetic Earthquake data based on the parameters of Greco et al. 2019, that simulates multiple seismic events over a period, notably including multiple magnitude 4-5 earthquakes, similar to our chosen period in Davao within our dataset.\n",
    "\n",
    "This gives us a sense of what magnitudes, intensity and 'energy' has been released during a certain period. This shall be correlated later on with the real data to determine 'when' exactly is this period in relation to the synthetic time steps generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1751dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_size, x='seismic_event', y='size')\n",
    "fig.update_traces(marker_line_width=0)\n",
    "fig.show()\n",
    "\n",
    "fig = px.bar(df_magnitudes, x='seismic_event', y='magnitude')\n",
    "fig.update_traces(marker_line_width=0)\n",
    "fig.show()\n",
    "\n",
    "fig = px.bar(df_intensities, x='seismic_event', y='intensity')\n",
    "fig.update_traces(marker_line_width=0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab62ee",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. For Synthetic Data Generation,\n",
    "- The study we found currently refines a parameter $f_b$ by only continually adding 0.1 until the magnitude of the $L \\times L$ matrix matches the magnitude formed by the original event with some error, \n",
    "- Are we allowed to perform a different method similar to newton iterations by adding delta, starting with 0.1, while the error keeps decreasing, when it increases after adding delta, halve it then go 'back' until the error decreases, repeat until we reach an error of epsilon\n",
    "\n",
    "This $f_b$ is used to help the L x L matrix made to match the magnitude of the real world condition, to better mimic the earthquake in Davao, this method is applied by (Greco et al., 2019)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8454af5",
   "metadata": {},
   "source": [
    "# References\n",
    "Boeing, G. (2014). Clustering to Reduce Spatial Data Set Size. https://geoffboeing.com/2014/08/\n",
    "clustering-to-reduce-spatial-data-set-size/\n",
    "\n",
    "Ferreira, D., Ribeiro, J., Oliveira, P., Pimenta, A., Freitas, R., Dutra, R., Papa, A., & Mendes, J. (2022). \n",
    "Spatiotemporal analysis of earthquake occurrence in synthetic and worldwide data. Chaos \n",
    "Solitons & Fractals, 165, 112814. https://doi.org/10.1016/j.chaos.2022.112814\n",
    "\n",
    "Greco, A., Pluchino, A., Barbarossa, L., Barreca, G., Caliò, I., Martinico, F., & Rapisarda, A. (2019). A \n",
    "New Agent-Based Methodology for the Seismic Vulnerability Assessment of Urban Areas. ISPRS \n",
    "International Journal of Geo-Information, 8(6). https://doi.org/10.3390/ijgi8060274"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
